# ðŸŒŸ Google Developer Group Cloud ZÃ¼rich ðŸŒŸ
Slides from my Google Developer Group Cloud ZÃ¼rich Presentation on Finetuning Open Source LLMs and AI Agents

## ðŸ“‘ Slides
- [2025-02-20 GDG Cloud Zurich Fine-Tuning Open-Source LLMs.pdf](./2025-02-20%20GDG%20Cloud%20Zurich%20Fine-Tuning%20Open-Source%20LLMs.pdf)

## ðŸ”— Links
### ðŸ‘‹ Follow/Connect with Erin
- [LinkedIn](https://www.linkedin.com/in/erinla/)
- [GitHub](https://github.com/giterinhub/)

### ðŸŽ“ Trainings
- [Andrej Karpathy: Deep Dive into LLMs like ChatGPT](https://youtu.be/7xTGNNLPyMI)
- [Huggingface: Generating Datasets](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned/blob/main/notebook.ipynb)
- [Huggingface: Fine-Tune a Large Language Model (LLM) for Function-Calling.](https://huggingface.co/learn/agents-course/bonus-unit1/introduction)
- [Huggingface NLP Course: Supervised Fine-Tuning](https://huggingface.co/learn/nlp-course/en/chapter11/1)
- [Google Cloud Skills Boost: Supervised Fine Tuning with Gemini for Question & Answering](https://www.cloudskillsboost.google/catalog_lab/31750)
- [Google and Kaggleâ€™s free GenAI Intensive course](https://blog.google/technology/developers/google-kaggle-genai-intensive/)

### ðŸš€ Projects shown/mentioned:
- [Github Blog: Five steps to Building an LLM Application](https://github.blog/ai-and-ml/llms/the-architecture-of-todays-llm-applications/)
- [Huggingface Agent Leaderboard](https://huggingface.co/spaces/galileo-ai/agent-leaderboard)
- [Google Gemma](https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures/)
- [Argilla distilabel](https://github.com/argilla-io/distilabel)
- [Huggingface Preference Datasets: ORPO DPO Mix](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k/)
- [Huggingface Preference Datasets: Ultrafeedback](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned/)
- [Weights & Biases](https://docs.wandb.ai/guides/integrations/huggingface/)
- [Model Management: Langtrace](https://docs.langtrace.ai/)

### ðŸ“š Literature shown/mentioned:
- [OpenAI: Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416)
- [OpenAI: Training LMâ€™s to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155)
- [KAIST AI: ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/pdf/2403.07691)
- [CeADAR Irelandâ€™s Centre for AI: The Ultimate Guide to Fine-Tuning LLMs](https://arxiv.org/pdf/2408.13296v1)
